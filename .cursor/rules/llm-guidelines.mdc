# LLM Implementation Guidelines

## Directory Structure

LLM-related code is organized in specific files:

- `summarization.js` - Main summarization implementations
- `summaryValidation.js` - Summary validation and quality checks
- `defaultPrompts.js` - Default prompt templates
- `promptUtils.js` - Prompt formatting utilities

## Key Implementation Pattern

Follow this standard structure for LLM-related functions:

```javascript
import { 
    get_settings, 
    getContext, 
    debug, 
    error, 
    toast,
    get_current_preset,
    get_current_connection_profile,
    generateRaw,
    validate_summary
} from './index.js';

async function summarizeMessage(options) {
    const { message, profile, preset } = options;
    
    if (!message || !message.mes) {
        debug("Invalid message for summarization");
        return null;
    }

    // Get system prompt from settings or defaults
    const systemPrompt = get_settings('summary_prompt') || defaultSummaryPrompt;
    
    // Format user prompt with message content
    const userPrompt = formatSummaryPrompt({
        message: message.mes,
        character: message.character,
        context: getSummaryContext(message)
    });

    debug("Summarizing message", { 
        messageLength: message.mes.length,
        character: message.character?.name 
    });

    try {
        // Set up API connection for summarization
        const currentPreset = await get_current_preset();
        const currentProfile = await get_current_connection_profile();
        
        await set_preset(preset || get_settings('completion_preset'));
        await set_connection_profile(profile || get_settings('connection_profile'));

        // Generate summary
        const result = await generateRaw({
            system: systemPrompt,
            prompt: userPrompt,
            max_tokens: get_settings('summary_max_tokens'),
            temperature: get_settings('summary_temperature')
        });

        // Validate summary if enabled
        if (get_settings('validate_summaries')) {
            const validation = await validate_summary(result.content, message);
            if (!validation.valid) {
                debug("Summary validation failed", validation.errors);
                return null;
            }
        }

        debug("Summary generated successfully", { 
            summaryLength: result.content.length 
        });

        return result.content;

    } catch (err) {
        error("Summarization failed", err);
        toast("Summarization failed", "error");
        return null;
    } finally {
        // Restore original preset and profile
        await set_preset(currentPreset);
        await set_connection_profile(currentProfile);
    }
}
```

## Best Practices

1. **Error Handling**:
   - Always wrap LLM calls in try-catch blocks
   - Use early returns for invalid inputs
   - Log errors with context for debugging
   - Show user-friendly error messages via toast

2. **Prompt Management**:
   - Keep system and user prompts separate
   - Use template functions for consistent formatting
   - Validate prompt length before sending
   - Include relevant context (character info, chat history)

3. **API Management**:
   - Always save and restore API settings
   - Handle rate limiting gracefully
   - Use appropriate timeouts for long operations
   - Support different API providers via connection profiles

4. **Validation**:
   - Validate summaries when enabled
   - Check for empty or malformed responses
   - Implement retry logic for transient failures
   - Use token counting to prevent context overflow

5. **Logging**:
   - Use debug() for detailed operation logs
   - Log input/output sizes for monitoring
   - Include relevant context in log messages
   - Use error() for actual errors with stack traces

6. **Performance**:
   - Implement debouncing for rapid requests
   - Cache results when appropriate
   - Use progress indicators for long operations
   - Batch operations when possible

## Common Patterns

### Summary Validation
```javascript
async function validateSummary(summary, originalMessage) {
    const validationPrompt = get_settings('validation_prompt');
    
    const result = await generateRaw({
        system: validationPrompt,
        prompt: `Summary: ${summary}\nOriginal: ${originalMessage.mes}`,
        max_tokens: 100,
        temperature: 0.1
    });

    return parseValidationResult(result.content);
}
```

### Context Building
```javascript
function buildSummaryContext(message, historyCount = 3) {
    const context = getContext();
    const messages = context.chat.slice(-historyCount);
    
    return messages.map(msg => ({
        role: msg.is_user ? 'user' : 'assistant',
        content: msg.mes,
        character: msg.character?.name
    }));
}
```

### Token Management
```javascript
function checkTokenLimits(prompt, maxTokens) {
    const tokenCount = count_tokens(prompt);
    const limit = getMaxContextSize();
    
    if (tokenCount + maxTokens > limit) {
        debug(`Token limit exceeded: ${tokenCount}/${limit}`);
        return false;
    }
    
    return true;
}
```
description:
globs:
alwaysApply: false
---
